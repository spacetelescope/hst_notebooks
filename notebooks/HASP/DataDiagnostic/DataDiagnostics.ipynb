{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HASP Data Diagnostic Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will walk you through how to examine the input spectra for the HASP `coadd` code and determine what was and was not included in the co-added data product output.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand the reasons an input spectrum may be rejected from a co-added data product\n",
    "- Learn how to examine output logs from `coadd` and header keywords from COS and STIS data products to determine which datasets were rejected\n",
    "- Know how to plot co-added spectra and the constituent datasets\n",
    "- Be able to re-run `coadd` with the default rejection criteria turned off to create custom co-additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "**0. [Introduction](#introduction)**\n",
    "\n",
    "**1. [Example 1: Two COS datasets rejected for different reasons](#cosweirddata)**\n",
    "\n",
    "\\- 1.1 [Obtaining Data Products](#data1)\n",
    "\n",
    "\\- 1.2 [Examining Output Logs](#logs1)\n",
    "\n",
    "\\- 1.3 [Plotting Constituent Spectra](#plots1)\n",
    "\n",
    "\\- 1.4 [Running `coadd`](#coadd1)\n",
    "\n",
    "**2. [Example 2: A STIS dataset with `POSTARG` offsets](#stisdithers)**\n",
    "\n",
    "\\- 2.1 [Obtaining Data Products](#data2)\n",
    "\n",
    "\\- 2.2 [Examining Output Logs](#logs2)\n",
    "\n",
    "\\- 2.3 [Running `coadd`](#coadd2)\n",
    "\n",
    "**3. [Example 3: A STIS dataset with rejected flux](#stisflux)**\n",
    "\n",
    "\\- 3.1 [Obtaining Data Products](#data3)\n",
    "\n",
    "\\- 3.2 [Examining Output Logs](#logs3)\n",
    "\n",
    "\\- 3.3 [Running `coadd`](#coadd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id = introduction></a>\n",
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Hubble Advanced Spectral Products (HASP) `coadd` code](https://github.com/spacetelescope/hasp) is a script that co-adds spectra of the same target within a program. This software is able to co-add data taken with the spectrographs onboard the [Hubble Space Telescope (HST)](https://www.stsci.edu/hst); the [Space Telescope Imaging Spectrograph (STIS)](https://www.stsci.edu/hst/instrumentation/stis) and the [Cosmic Origins Spectrograph (COS)](https://www.stsci.edu/hst/instrumentation/cos). The [Hubble Spectroscopic Legacy Archive (HSLA)](https://archive.stsci.edu/missions-and-data/hst/hasp) uses this script to co-add these instrumentsâ€™ data from [The Mikulski Archive for Space Telescopes (MAST)](https://archive.stsci.edu/) to create high-quality spectra with a broad wavelength coverage (whenever possible from the ultraviolet to the near-infrared) that is publicly available for the scientific community. The script first co-adds the observations for each grating for a given visit, then it combines all gratings for the observation set. Finally, it co-adds the spectra of each observation set in the program to produce a fully co-added spectra for each target in a program. [Check out the COS 2024-1 ISR for more information about the HASP script and products](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/cos/documentation/instrument-science-reports-isrs/_documents/ISR2024-01.pdf).\n",
    "\n",
    "The data fed into the `coadd` code from each program are selected via a MAST archive search query. This query performs some filtering at the same time and rejects certain bad quality datasets. Then, the `coadd` script itself performs another series of checks on the input spectra before computing the co-added data products. Lastly, while `coadd` is running, it will check the flux of the input data in each wavelength bin for a given mode against an initial coadd that includes all input data. If the median flux of an input spectrum is lower than a given threshold against the co-add, it will be removed. `coadd` will iterate until no more spectra are rejected.\n",
    "\n",
    "There are several different reasons an exposure could be rejected before the co-addition is running. These fall into three categories. First, the data in a given program may have quality issues that lead to its removal. Second, there are some observation configurations that `coadd` is not equipped to handle. Last, the target itself may pose an issue, such as for moving targets, or the target flux may be intrinsically variable, making the `coadd` flux filter reject good spectra. Finer details of these rejection criteria are summarized below:\n",
    "\n",
    "| Reason                                                                                                             | Modes Effected |\n",
    "|--------------------------------------------------------------------------------------------------------------------|----------------|\n",
    "|**Observing Issues**                                                |                |\n",
    "| Guide star acquisition failures                                                                                    | Any            |\n",
    "| Observatory or detector failure events                                                                             | Any            |\n",
    "| EXPFLAG (exposure data quality flag) header keyword is anything other than 'NORMAL'                                | Any            |\n",
    "| EXPTIME (exposure time) is zero seconds                                                                            | Any            |\n",
    "| Actual exposure time is less than 80% of the planned exposure time                                                 | Any            |\n",
    "| FGSLOCK (fine guidance system lock) is not 'FINE', i.e., guide star tracking was not locked                        | Any            |\n",
    "| SHUTTER is closed                                                                                                  | COS            |\n",
    "| **Observation Parameters**                                             |                |\n",
    "| POSTARG1 != 0.0, i.e., there is a pointing offset                                                                  | Any            |\n",
    "| POSTARG2 != 0.0 and P1_PURPS != 'DITHER', i.e., there is a pointing offset not in the disperson direction          | STIS           |\n",
    "| PATTERN1 = STIS-PERP-TO-SLIT and P1_FRAME = POS-TARG, i.e., there is a cross-dispersion direction pointing pattern | STIS           |\n",
    "| P1_PURPS = MOSAIC, i.e., there is a mosaic pointing offset pattern                                                 | STIS           |\n",
    "| OPT_ELEM (grating) = PRISM                                                                                         | STIS           |\n",
    "| APERTURE = BOA (Bright Object Aperture)                                                                            | COS            |\n",
    "| For the COS/NUV grating G230L, stripe C spectra are rejected due to vignetting                                     | COS/NUV        |\n",
    "| **Target Parameters**                                               |                |\n",
    "| Moving targets (MTFLAG = True)                                                                                     | Any            |\n",
    "| Variable targets*                                                                                              | Any            |\n",
    "| Extended targets*                                                                                               | Any            |\n",
    "\n",
    "*These are not rejected by default, but some exposures may be removed by the code's flux checking routine.\n",
    "\n",
    "The HSLA team chose to reject these cases after careful analysis, but understand there are always some exceptions to these rules that we do not handle. This custom co-addition notebook will show users how to find out why a dataset was rejected and how to produce their own co-adds in cases where the data are still valuable.\n",
    "\n",
    "_Please note that the format and text in the log file may change slightly as new code builds are released. This notebook will be updated in Spring 2024 to reflect any text changes in the logs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imports\n",
    "We will be using multiple libraries to retrieve and analyze data. We will use:\n",
    "* `astroquery.mast Observations` to download COS and STIS data\n",
    "* `pathlib.Path` to create product and data directories\n",
    "* `matplotlib.pyplot` to plot spectra\n",
    "* `numpy` to perform calculations and array manipulation\n",
    "* `astropy.io fits` to work with FITS files\n",
    "* `astropy.table Table` to work with FITS tables\n",
    "* `glob` to work with multiple files in our directories\n",
    "* `os` to interact with the operating system\n",
    "* `shutil` to perform directory and file operations\n",
    "  \n",
    "We recommend creating a HASP-specific `conda` environment when co-adding spectra. You can checkout our [Setup.ipynb](https://github.com/spacetelescope/hst_notebooks/tree/main/notebooks/HASP/Setup) notebook to create such an environment. Alternatively, you can also download the required dependencies to run this notebook with the terminal command:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "This will download the dependencies that are necessary to run this current notebook. Let's import all of our packages that we will use in this notebook and print our `conda` environment by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.table import vstack\n",
    "from astroquery.mast import Observations\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.style.use('seaborn-v0_8-colorblind')\n",
    "\n",
    "print(\"Currently active conda environment:\", os.environ.get(\"CONDA_PREFIX\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure the environment that contains the HASP script dependencies is activated or you download the dependencies listed in the `requirements.txt` file, otherwise **_you will NOT be able to run the co-add code._**\n",
    "\n",
    "We will define a function that will be utilized throughout the notebook when downloading MAST data using `astroquery`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_files(data_path):\n",
    "    '''\n",
    "    Consolidate all files to single directory; necessary for HASP script run.\n",
    "    ---------------\n",
    "    Input:\n",
    "    str data_path : ./mastDownload/HST folders paths; files to be moved here\n",
    "    ---------------\n",
    "    Output:\n",
    "    None. Files moved to data_path and data_path/products.\n",
    "    The ./mastDownload/HST directory is deleted.\n",
    "    '''\n",
    "    # The path to all obs_id folders\n",
    "    mast_path = f\"{data_path}/mastDownload/HST/\"\n",
    "    try:\n",
    "        # Check if mastDownload exists\n",
    "        if not os.path.exists(mast_path):\n",
    "            print(f\"Directory {mast_path} doesn't exist.\")\n",
    "            return\n",
    "        # Get a list of the obs_id paths in mastDownload\n",
    "        obs_id_dirs = os.listdir(mast_path)\n",
    "        # Iterate through each obs_id folder and move the files\n",
    "        for obs_id in obs_id_dirs:\n",
    "            obs_id_path = os.path.join(mast_path, obs_id)\n",
    "            files = glob.glob(obs_id_path + \"/*fits\")\n",
    "            for file in files:\n",
    "                file_path = Path(file)\n",
    "                new_path = data_path / file_path.name\n",
    "                shutil.move(file, new_path)\n",
    "        # Now we can remove the mastDownload directory\n",
    "        if os.path.exists(mast_path):\n",
    "            shutil.rmtree(f\"{data_path}/mastDownload\")\n",
    "        # Now moving all coadd products to /data_path/products\n",
    "        product_path = Path(f\"{data_path}/products/\")\n",
    "        if not os.path.exists(product_path):\n",
    "            print(f\"Directory {product_path} doesn't exist.\")\n",
    "            return\n",
    "        coadd_files = glob.glob(f\"{data_path}/*cspec.fits\")\n",
    "        for file in coadd_files:\n",
    "            file_path = Path(file)\n",
    "            new_path = product_path / file_path.name\n",
    "            shutil.move(file, new_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = cosweirddata></a>\n",
    "# Example 1: Two COS datasets rejected for different reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = data1></a>\n",
    "### 1.1 Obtaining Data Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be looking at Program ID 12715, which observed four flux standard white dwarf stars with COS.\n",
    "We will use `astroquery` to download the calibrated and coadded datasets for this program. For a more in-depth tutorial on downloading this data, please check out the [CoaddTutorial.ipynb](https://github.com/spacetelescope/hst_notebooks/tree/main/notebooks/HASP/CoaddTutorial) notebook in this repository.\n",
    "\n",
    "We will create a folder for the `X1D` and `X1DSUM` products, called `./12715`, and a subfolder called `products`, which will store the coadded data. The log file for this coadd is currently not available for download on MAST, but is in this repository. It will be available for download in a future build. This log file is called `HASP_12715.out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories for our data and coadded products\n",
    "datadir_ex1 = Path(\"./12715/\")\n",
    "productsdir_ex1 = Path(\"./12715/products/\")\n",
    "\n",
    "datadir_ex1.mkdir(exist_ok=True)\n",
    "productsdir_ex1.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our datasets for this program by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying and downloading calibrated products\n",
    "query_ex1 = Observations.query_criteria(\n",
    "    proposal_id=12715, \n",
    "    provenance_name=\"CALCOS\"\n",
    ")\n",
    "\n",
    "prodlist_ex1 = Observations.get_product_list(\n",
    "    query_ex1\n",
    ")\n",
    "\n",
    "prodlist_ex1 = Observations.filter_products(\n",
    "    prodlist_ex1,\n",
    "    project=[\"CALCOS\"],\n",
    "    productSubGroupDescription=[\"X1D\", \"X1DSUM\"]\n",
    ")\n",
    "\n",
    "# Querying and downloading coadded products\n",
    "query_ex1_coadds = Observations.query_criteria(\n",
    "    proposal_id=12715, \n",
    "    provenance_name=\"HASP\"\n",
    ")\n",
    "\n",
    "prodlist_ex1_coadds = Observations.get_product_list(\n",
    "    query_ex1_coadds\n",
    ")\n",
    "\n",
    "prodlist_ex1_coadds = Observations.filter_products(\n",
    "    prodlist_ex1_coadds, \n",
    "    productType=\"SCIENCE\",\n",
    "    productSubGroupDescription=\"CSPEC\"\n",
    ")\n",
    "\n",
    "# Combining the two product lists\n",
    "combined_ex1 = vstack([prodlist_ex1, prodlist_ex1_coadds])\n",
    "\n",
    "# Downloading the products\n",
    "Observations.download_products(\n",
    "    combined_ex1,\n",
    "    download_dir=str(datadir_ex1)\n",
    ")\n",
    "\n",
    "# Organizing the files \n",
    "consolidate_files(datadir_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use `glob` to make a list of every `X1D` file available in MAST for this PID. We will print out a table of some table information on the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allfiles = glob.glob(os.path.join(datadir_ex1, '*_x1d.fits'))\n",
    "print('rootname  target  visit  grating')\n",
    "\n",
    "for x1d in sorted(allfiles):\n",
    "    hdr0 = fits.getheader(x1d, ext=0)\n",
    "    print(hdr0['rootname'], hdr0['targname'],\n",
    "          hdr0['obset_id'], hdr0['opt_elem'])\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"N files from MAST = {len(allfiles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = logs1></a>\n",
    "### 1.2 Examining Output Logs and Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets open the `coadd` output log to see which files were used in the coadd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up path to the coadd log file\n",
    "logfile = './logfiles/HASP_12715.out'\n",
    "\n",
    "with open(logfile, 'r') as f:\n",
    "    for line in f:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the introduction, files can be removed from the co-added products in three steps. First, some files are excluded via a MAST archive query before they are even fed into `coadd`. Everything in the log file above the line that says `[124 rows x 10 columns]` is output from this query. So, in this example, we can see that 124 of the 125 files from MAST were fed into the `coadd` script. The rest of the log file gives the output from `coadd` itself. Unlike the MAST query, the `coadd` script will print a note when a file is removed, giving both the filename and the reason. Typically, data with observing issues are removed by the MAST query, while observation or target parameter issues are removed in `coadd`. Looking through the log file, we can see 9 other files were removed by `coadd` in this program:\n",
    "\n",
    "```\n",
    "File ./lbui51l6q_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51lcq_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51lrq_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51ltq_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51lvq_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51m5q_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51m7q_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51mbq_x1d.fits removed from products because FGSLOCK = FINE/GYRO \n",
    "File ./lbui51mdq_x1d.fits removed from products because FGSLOCK = FINE/GYR0 \n",
    "```\n",
    "\n",
    "This program does not have any data that is removed by the `coadd` flux checker. If a dataset is removed, it will be printed with lines such as:\n",
    "\n",
    "```\n",
    "Using a maximum SNR of 20 in flux-based filtering \\\n",
    "Segment #1 from file ./lede16w8q_x1d.fits has scaled median = -54.04932196288165 \\\n",
    "Removing file ./lede16w8q_x1d.fits from product\n",
    "```\n",
    "\n",
    "Next, we will write a function to parse the output logs and return lists of the files that were rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rejects(logpath, pid, listofallfiles):\n",
    "    \"\"\"\n",
    "    This function parses the coadd output log file to make a list of files that\n",
    "    were input to the coadd script. The function uses the PID to find the lines\n",
    "    that are listed like so in the log file:\n",
    "        Creating list of unique modes from these files:\n",
    "        ./lbui01e7s_x1d.fits WD0947+857 COS FUV G140L PSA 12715 (12715, '01')\n",
    "        ...\n",
    "        ./lbui50r5q_x1d.fits WD0308-565 COS FUV G140L PSA 12715 (12715, '50')\n",
    "    Then, it compares that to the list of every file from the program that you\n",
    "    download from MAST. The difference in the two lists are the rejected files.\n",
    "\n",
    "    It looks through the log for files that were rejected by the flux checker.\n",
    "    This searches for lines that are printed like so in the log file:\n",
    "        Removing file ./lede16w8q_x1d.fits from product\n",
    "    Because a file can be removed more than once in the creation of different\n",
    "    level data products, the list returned will only include unique entries.\n",
    "\n",
    "    Args:\n",
    "        logname (string): Path to the coadd output log file\n",
    "        pid (string): Proposal ID of the program\n",
    "        listofallfiles (list of strings): list of every path+filename program\n",
    "\n",
    "    Returns:\n",
    "        prerejectedfiles (list of str): list of files rejected by MAST query\n",
    "                                        or coadd before computations began\n",
    "        fluxrejectedfiles (list of str): list of files rejected by flux checker\n",
    "    \"\"\"\n",
    "\n",
    "    # Open output log and make a list of rootnames that were used in the coadd\n",
    "    # Also search for rootnames that were rejected by the flux checker\n",
    "    with open(logpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    coaddedfiles = []\n",
    "    fluxrejectedfiles = []\n",
    "    for line in lines:\n",
    "        if ('./' in line.strip()) and (pid in line.strip()):\n",
    "            coaddedfiles.append(line.split()[0].split('/')[1])\n",
    "        if 'Removing file' in line.strip():\n",
    "            fluxrejectedfiles.append(line.split()[2].split('/')[1])\n",
    "\n",
    "    # Compare coadd list against list of all files in PID downloaded from MAST\n",
    "    prerejectedfiles = []\n",
    "    for filepath in listofallfiles:\n",
    "        root = filepath.split('/')[-1]\n",
    "        if root not in coaddedfiles:\n",
    "            prerejectedfiles.append(root)\n",
    "\n",
    "    return prerejectedfiles, np.unique(fluxrejectedfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run find_rejects and print the output\n",
    "listofprerejects, listoffluxrejects = find_rejects(logfile, '12715', allfiles)\n",
    "print('Files removed before co-addition:')\n",
    "[print(f\"{file}\") for file in sorted(listofprerejects)]\n",
    "print(f'Files removed by flux checker: {sorted(listoffluxrejects)}')\n",
    "print(f'Number of files removed before co-addition = {len(listofprerejects)}')\n",
    "print(f'Number of files removed by flux checker = {len(listoffluxrejects)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see 10 files were removed before the co-addition computations were started, and none were removed during the run by the flux checker. We already knew the files in Visit 51 were removed by `coadd` (and why) by looking at the output log above. The file from Visit 11 (`lbui11faq_x1d.fits`) was removed by the MAST query, but we are not given a reason in the log. We can investigate the rejections from both visits further by inspecting the header keywords of the input spectra.\n",
    "\n",
    "In the following cell, we make a function to read the file headers for each input spectrum. Most of the reasons for rejection that are listed in the [Introduction](#introduction) section can be found either in the primary or first extension headers. This function can be used to inspect both COS and STIS extracted data, both `x1d` or `sx1` files. Note that some keywords, such as those describing the offset patterns, are only included in STIS files.\n",
    "\n",
    "There are a few other keywords not listed in the [Introduction](#introduction) that are useful to assess as well. These include the quality comments (`QUALCOM1`, `QUALCOM2`, `QUALCOM3`), the file date (`DATE`), and calibration software version (`CAL_VER`). \n",
    "\n",
    "The quality comments can hold information about the quality of a dataset. For example, they may have phrases like `Guide star acquisition failed. Actual guide mode is gyro` or `COS internal shutter closed. No counts in exposure`. If there is nothing to note for the dataset, the quality comments will be blank, but note that sometimes this can be inaccurate, as it relies on PIs to file problem reports. For STIS, these keys are in the `x1d` or `sx1` file headers. For COS, they are located in the `x1dsum` files that CalCOS creates, which are the sum of `FP-POS` exposures for a given observing mode in each visit. While these keywords do exist in COS `x1d` files, they will only be populated with comments in the `x1dsums`. To find the `x1dsum` file for these observations, we can use the `ASN_ID` keyword from the `x1ds`.\n",
    "\n",
    "The date the file was written and the version of the software used for calibration are useful for finding datasets that are archived \"statically\" in MAST, meaning they are always excluded from re-calibration because doing so will crash the latest versions of the calibration pipeline. There are only a handful of statically archived datasets for COS and STIS. The reasons they need to be designated as such are usually due to detector or spacecraft issues that need very specialized processing, and this is not always captured in the quality comments. Therefore, we do not recommend using these datasets in co-adds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readheaders(prerejectedfiles, datadir):\n",
    "    \"\"\"\n",
    "    This function goes through a list of the pre-rejected files and prints the \n",
    "    x1d header information about data quality. Some of these quality comments \n",
    "    can describe issues that might have occured during the observation. \n",
    "    Args:\n",
    "        prerejectedfiles (list of strings): list of rejected filenames\n",
    "        datadir (str): path to x1d/sx1/x1dsum files\n",
    "    \"\"\"\n",
    "\n",
    "    for badfile in sorted(prerejectedfiles):\n",
    "        badfilepath = os.path.join(datadir, badfile)\n",
    "        print(badfilepath)\n",
    "        # Open the 0th and 1st ext. headers to get data quality info\n",
    "        hdr0 = fits.getheader(badfilepath, ext=0)\n",
    "        hdr1 = fits.getheader(badfilepath, ext=1)\n",
    "\n",
    "        # Note that header keywords differ between COS and STIS\n",
    "        ins = hdr0['INSTRUME']\n",
    "\n",
    "        # print lots of keywords\n",
    "        print(f\"Exposure time = {hdr1['EXPTIME']}\")\n",
    "        # These keywords doesn't exist in STIS data\n",
    "        if ins == 'COS':\n",
    "            print(f\"Planned exposure time = {hdr1['PLANTIME']}\")\n",
    "            print(f\"Shutter = {hdr0['SHUTTER']}\")\n",
    "        print(f\"Aperture used = {hdr0['APERTURE']}\")\n",
    "        print(f\"Grating used = {hdr0['OPT_ELEM']}\")\n",
    "        print(f\"Exposure flag = {hdr1['EXPFLAG']}\")\n",
    "        print(f\"Fine guiding lock = {hdr1['FGSLOCK']}\")\n",
    "        print(f\"POSTARG1 / POSTARG2 = {hdr0['POSTARG1']} / {hdr0['POSTARG2']}\")\n",
    "        # These keywords are not present in COS x1d data\n",
    "        if ins == 'STIS':\n",
    "            print(f\"Offset pattern = {hdr0['PATTERN1']}\")\n",
    "            print(f\"P1 frame = {hdr0['P1_FRAME']}\")\n",
    "            print(f\"P1 purpose = {hdr0['P1_PURPS']}\")\n",
    "            print('Quality comments')\n",
    "            print(f\"    COM1 = {hdr0['QUALCOM1']}\")\n",
    "            print(f\"    COM2 = {hdr0['QUALCOM2']}\")\n",
    "            print(f\"    COM3 = {hdr0['QUALCOM3']}\")\n",
    "        # Find the x1dsum for the COS data to get the quality comments\n",
    "        if ins == 'COS':\n",
    "            asn_id = hdr0['ASN_ID']\n",
    "            x1dsum = os.path.join(datadir, asn_id.lower() + '_x1dsum.fits')\n",
    "            if os.path.isfile(x1dsum):\n",
    "                print(f'Quality comments from {x1dsum}')\n",
    "                print('    COM1 = ' + fits.getval(x1dsum, 'QUALCOM1', ext=0))\n",
    "                print('    COM2 = ' + fits.getval(x1dsum, 'QUALCOM2', ext=0))\n",
    "                print('    COM3 = ' + fits.getval(x1dsum, 'QUALCOM3', ext=0))\n",
    "            else:\n",
    "                print('Quality comments: No x1dsum available')\n",
    "        if hdr0['MTFLAG'] != 'T':\n",
    "            print('Moving target? No')\n",
    "        else:\n",
    "            print('Moving target? Yes')\n",
    "        print(f\"Date file was written = {hdr0['DATE']}\")\n",
    "        print(f\"Version of calibration software = {hdr0['OPUS_VER']}\")\n",
    "        print('')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readheaders(listofprerejects, datadir_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the output from the header keywords printed above, we see there is no `x1dsum` to inspect for `lbui11faq_x1d.fits`. This is unusual for COS data! We can also see that the date the file was written was in 2012, and that the version of the calibration software used to calibrate this data was from 2012. All together, this indicates that this dataset is statically archived in MAST, and so we don't recommend using it in a co-add. To see the latest versions of the calibration pipeline software, see the [HST Data Processing (HSTDP) github page](https://github.com/astroconda/astroconda-releases/tree/master/caldp).\n",
    "\n",
    "We can also see that the quality comments are all blank for the Visit 51 datasets. We know from the output log file that these data were rejected because some of the exposure was observed using gyro guiding, which is less accurate than fine guiding that tracks targets with guide stars. Because there are no quality comments listed to indicate a target acquisition failure, this data may still be useable, and we'll explore that next. This data is of target `WD0308-565`, which was also observed in Visit 50 with the same observing setup. We can compare the fluxes of the two visits to see if Visit 51's data is alright for co-addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = plots1></a>\n",
    "### 1.3 Plotting Constituent and Co-added Spectra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to bin the data so it will plot more clearly\n",
    "def downsample_1d(myarr, factor):\n",
    "    \"\"\"\n",
    "    Downsample a 1D array by averaging over *factor* pixels.\n",
    "    Crops right side if the shape is not a multiple of factor.\n",
    "    Got this specific function from \"Adam Ginsburg's python codes\" on agpy\n",
    "\n",
    "    myarr : numpy array\n",
    "    factor : how much you want to rebin the array by\n",
    "    \"\"\"\n",
    "    xs = myarr.shape[0]\n",
    "    crarr = myarr[:xs-(xs % factor)]\n",
    "    dsarr = crarr.reshape(-1, factor).mean(axis=1)\n",
    "    return dsarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'hst_12715_cos_wd0308-565_cg140l-g130m-g160m_lbui50_cspec.fits'\n",
    "\n",
    "# Set up a path to the visit 50 level co-added data product for this target\n",
    "coaddfile = os.path.join(datadir_ex1, 'products', filename)\n",
    "coadddata = fits.getdata(coaddfile)\n",
    "\n",
    "# Proactively set sample factor to 6, which is size of COS resolution element\n",
    "# For STIS, this is 2\n",
    "samplefactor = 6\n",
    "\n",
    "# The visit we want to plot\n",
    "visit_to_plot = \"51\"\n",
    "\n",
    "# Get the wavelength and flux data for the co-added file\n",
    "wavelength = coadddata['wavelength'][0]\n",
    "flux = coadddata['flux'][0]\n",
    "\n",
    "# Set up the plot\n",
    "plt.close()\n",
    "plt.figure(1)\n",
    "\n",
    "# Plot the constiuent spectra\n",
    "for x1d in sorted(allfiles):\n",
    "    # First check that the file is for the correct visit\n",
    "    visit_id = fits.getval(x1d, 'obset_id', ext=0)\n",
    "    if visit_id == visit_to_plot:\n",
    "        x1ddata = fits.getdata(x1d)\n",
    "        subwave = x1ddata['wavelength'][0]\n",
    "        subflux = x1ddata['flux'][0]\n",
    "        plt.plot(downsample_1d(subwave, samplefactor),\n",
    "                 downsample_1d(subflux, samplefactor),\n",
    "                 label=fits.getval(x1d, 'rootname', ext=0),\n",
    "                 alpha=0.7)\n",
    "\n",
    "# Overplot the co-add\n",
    "plt.plot(downsample_1d(wavelength, samplefactor),\n",
    "         downsample_1d(flux, samplefactor),\n",
    "         c='black',\n",
    "         label='visit 50 co-add',\n",
    "         alpha=0.7)\n",
    "\n",
    "# Format the plot by adding titles\n",
    "targ = fits.getval(coaddfile, 'TARGNAME', ext=0)\n",
    "pid = fits.getval(coaddfile, 'PROPOSID', ext=0)\n",
    "ins = fits.getval(coaddfile, 'INSTRUME', ext=0)\n",
    "\n",
    "plt.title(f'{targ} - {ins} - PID {pid}')\n",
    "plt.xlabel(r'Wavelength [$\\AA$]')\n",
    "plt.ylabel(r'Flux [$erg\\ s^{-1}\\ cm^{-2}\\ \\AA^{-1}$]')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot below\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the interactive features in the top left corner to zoom in on a region of continuum in the plot above. The zoom button looks like a square and allows you to select a rectangular region on the plot to enlarge. We see no systematic difference in the data from Visit 51 compared to the co-added data in Visit 50, so let's add the Visit 51 data into the co-add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = coadd1></a>\n",
    "### 1.4 Re-running `coadd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which data we want to use in the custom co-add, we must create a new directory with all the data from Visits 50 and 51. We will feed `coadd` the data from this new directory from its `wrapper` script. Running `coadd` this way essentially skips the filtering that the MAST query applies, but `coadd` itself still has some internal data quality checks, as mentioned above, so we will need to turn those off. If there is data you still want `coadd` exclude that was filtered before, be sure to not put those in the data directory!\n",
    "\n",
    "The following cell is the call to `coadd` via its `wrapper`. The `-i` parameter is the input directory you just made. `-o` is the directory that will contain the newly created co-added products. The `-k` turns off the data quality filtering. There is more information about this in our [Setup.ipynb](https://github.com/spacetelescope/hst_notebooks/tree/main/notebooks/HASP/Setup) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the path to your new data directory\n",
    "finaldatadir = os.path.join(datadir_ex1, 'newcoadddata')\n",
    "os.makedirs(finaldatadir, exist_ok=True)\n",
    "\n",
    "# Copy all the data from visits 50 and 51 into it\n",
    "filestocopy = glob.glob(os.path.join(datadir_ex1, '*50*_x1d.fits'))\\\n",
    "            + glob.glob(os.path.join(datadir_ex1, '*51*_x1d.fits'))\n",
    "\n",
    "[shutil.copy(file, finaldatadir) for file in filestocopy]\n",
    "\n",
    "# Make an output directory\n",
    "os.makedirs(os.path.join(datadir_ex1, 'newcoadddata', 'products'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call `coadd`, we use the `!` to run from the command line. The directories here must be printed out in full - don't use variable names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!swrapper -i ./12715/newcoadddata -o ./12715/newcoadddata/products -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replot the co-added data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the older plot\n",
    "plt.close()\n",
    "plt.figure(2)\n",
    "\n",
    "coadd_filename = 'hst_12715_cos_wd0308-565_cg140l-g130m-g160m_lbui_cspec.fits'\n",
    "\n",
    "# Plot the old coadd\n",
    "oldcoaddfile = os.path.join(datadir_ex1, 'products', coadd_filename)\n",
    "oldcoadddata = fits.getdata(oldcoaddfile)\n",
    "oldwavelength = oldcoadddata['wavelength'][0]\n",
    "oldflux = oldcoadddata['flux'][0]\n",
    "\n",
    "plt.plot(downsample_1d(oldwavelength, samplefactor),\n",
    "         downsample_1d(oldflux, samplefactor),\n",
    "         label='old coadd',\n",
    "         color=\"red\",\n",
    "         alpha=0.7)\n",
    "\n",
    "# Plot the new coadd\n",
    "newcoaddfile = os.path.join(finaldatadir, 'products', coadd_filename)\n",
    "newcoadddata = fits.getdata(newcoaddfile)\n",
    "newwavelength = newcoadddata['wavelength'][0]\n",
    "newflux = newcoadddata['flux'][0]\n",
    "\n",
    "plt.plot(downsample_1d(newwavelength, samplefactor),\n",
    "         downsample_1d(newflux, samplefactor),\n",
    "         label='new coadd',\n",
    "         color=\"blue\",\n",
    "         alpha=0.7)\n",
    "\n",
    "targ = fits.getval(coaddfile, 'TARGNAME', ext=0)\n",
    "pid = fits.getval(coaddfile, 'PROPOSID', ext=0)\n",
    "ins = fits.getval(coaddfile, 'INSTRUME', ext=0)\n",
    "\n",
    "plt.title(f'{targ} - {ins} - PID {pid}')\n",
    "plt.xlabel(r'Wavelength [$\\AA$]')\n",
    "plt.ylabel(r'Flux [$erg\\ s^{-1}\\ cm^{-2}\\ \\AA^{-1}$]')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot below\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the signal-to-noise to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the older plot\n",
    "plt.close()\n",
    "plt.figure(3)\n",
    "\n",
    "# Plot the old coadd SNR\n",
    "oldsnr = oldcoadddata['snr'][0]\n",
    "plt.plot(downsample_1d(oldwavelength, samplefactor),\n",
    "         downsample_1d(oldsnr, samplefactor),\n",
    "         label='old coadd',\n",
    "         color=\"red\",\n",
    "         alpha=0.7)\n",
    "\n",
    "# Plot the new coadd SNR\n",
    "newsnr = newcoadddata['snr'][0]\n",
    "plt.plot(downsample_1d(newwavelength, samplefactor),\n",
    "         downsample_1d(newsnr, samplefactor),\n",
    "         label='new coadd',\n",
    "         color=\"blue\",\n",
    "         alpha=0.7)\n",
    "\n",
    "plt.title(f'{targ} - {ins} - PID {pid}')\n",
    "plt.xlabel(r'Wavelength [$\\AA$]')\n",
    "plt.ylabel(r'Signal-to-Noise')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot below\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from the plot above that the SNR is much improved by adding the Visit 51 data into the co-add!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = stisdithers></a>\n",
    "# Example 2: A STIS dataset with POSTARG offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = data2></a>\n",
    "### 2.1 Obtaining Data Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next example, we will look at Program ID 16655, a STIS program that observed the star [Betelgeuse](http://simbad.cds.unistra.fr/simbad/sim-basic?Ident=Betelgeuse&submit=SIMBAD+search) with the `E230M` grating centered on the target and at `POSTARGs` `+/-0.25 mas` and `+/- 0.5 mas`. Each visit in the programs contains the same spatial scan in the pattern `+0.5 mas`, `-0.5 mas`, `centered`, `+0.25 mas`, `-0.25 mas`. `coadd` will co-add the centered datasets in each visit, but will reject the other datasets with `POSTARGs`. \n",
    "\n",
    "We will again use `astroquery` to download the dataproducts for this program. We will create a folder for the `X1D` products, called `./16655`, and a subfolder called `products`, which will store the downloaded coadded data. The log file for this program is named `HASP_16655.out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories for our data and coadded products\n",
    "datadir_ex2 = Path(\"./16655/\")\n",
    "productsdir_ex2 = Path(\"./16655/products/\")\n",
    "\n",
    "datadir_ex2.mkdir(exist_ok=True)\n",
    "productsdir_ex2.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying and downloading calibrated products\n",
    "query_ex2 = Observations.query_criteria(\n",
    "    proposal_id=16655, \n",
    "    provenance_name=\"CALSTIS\",\n",
    "    filters=\"E230M\",\n",
    "    target_name=\"HD39801\"\n",
    ")\n",
    "\n",
    "prodlist_ex2 = Observations.get_product_list(\n",
    "    query_ex2\n",
    ")\n",
    "\n",
    "prodlist_ex2 = Observations.filter_products(\n",
    "    prodlist_ex2,\n",
    "    project=[\"CALSTIS\"],\n",
    "    productSubGroupDescription=[\"X1D\"]\n",
    ")\n",
    "\n",
    "# Querying and downloading coadded products\n",
    "query_ex2_coadds = Observations.query_criteria(\n",
    "    proposal_id=16655, \n",
    "    provenance_name=\"HASP\",\n",
    "    filters=\"E230M\",\n",
    "    target_name=\"HD39801\"\n",
    ")\n",
    "\n",
    "prodlist_ex2_coadds = Observations.get_product_list(\n",
    "    query_ex2_coadds\n",
    ")\n",
    "\n",
    "prodlist_ex2_coadds = Observations.filter_products(\n",
    "    prodlist_ex2_coadds, \n",
    "    productType=\"SCIENCE\",\n",
    "    productSubGroupDescription=\"CSPEC\"\n",
    ")\n",
    "\n",
    "# Combining the two product lists\n",
    "combined_ex2 = vstack([prodlist_ex2, prodlist_ex2_coadds])\n",
    "\n",
    "# Downloading the products\n",
    "Observations.download_products(\n",
    "    combined_ex2,\n",
    "    download_dir=str(datadir_ex2)\n",
    ")\n",
    "\n",
    "# Organizing the files \n",
    "consolidate_files(datadir_ex2)\n",
    "\n",
    "# Removing files not needed for example\n",
    "files_to_remove = glob.glob(f\"{datadir_ex2}/oen75*\")\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(file)\n",
    "\n",
    "os.remove(f\"{productsdir_ex2}/hst_16655_stis_hd39801_e230m_oen751_cspec.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print some table information from our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allfiles = glob.glob(os.path.join(datadir_ex2, '*_x1d.fits'))\n",
    "print('rootname  target  visit  grating')\n",
    "\n",
    "for x1d in sorted(allfiles):\n",
    "    hdr0 = fits.getheader(x1d, ext=0)\n",
    "    print(hdr0['rootname'], hdr0['targname'],\n",
    "          hdr0['obset_id'], hdr0['opt_elem'])\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(f'N files from MAST = {len(allfiles)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = logs2></a>\n",
    "### 2.2 Examining Output Logs and Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the log next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up path to the coadd log file\n",
    "logfile = './logfiles/HASP_16655.out'\n",
    "\n",
    "with open(logfile, 'r') as f:\n",
    "    for line in f:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that two files were removed by the MAST query and that many other files were removed from the co-add because of `POSTARG` offsets, as expected. There are also some files that were removed later by the flux checker. Let's see which files, and how many were rejected next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the find_rejects function to make list of pre-rejected files\n",
    "listofprerejects, listoffluxrejects = find_rejects(logfile, '16655', allfiles)\n",
    "print(f'Files removed before co-addition: {sorted(listofprerejects)}')\n",
    "print(f'Files removed by flux checker: {sorted(listoffluxrejects)}')\n",
    "print(f'Number of files removed before co-addition = {len(listofprerejects)}')\n",
    "print(f'Number of files removed by flux checker = {len(listoffluxrejects)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, look in the `x1d` headers to find the exposure information and quality comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "readheaders(listofprerejects, datadir_ex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through this output, we can see there are two datasets from Visit 01 that had failed target acquisitions and were removed from the co-add. The quality comments tell us the aperture door was closed through the whole exposure, so no useful data was taken. The rest of the datasets were rejected because of the `POSTARG` offsets, as we already knew, and there are no other quality issues.\n",
    "\n",
    "This leaves only four datasets that were included in the call to create the default versions of the data products: `oen702030_x1d.fits`, `oen703030_x1d.fits`, `oen703030_x1d.fits`, and `oen705030_x1d.fits`. However, the data in Visits 2, 3, and 5 were later removed by the flux checker. Since this program was designed to probe Betelgeuse's recent flux variability, it makes sense that some of the fluxes may indeed be flagged for removal. In the custom co-add runs we perform next, we can set flags to ignore the `POSTARG` and flux filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = coadd1></a>\n",
    "### 2.3 Running `coadd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the program is such that each visit observed Betelgeuse at many `POSTARG` positions using the same gratings. The spatial scanning pattern is uniform, and so a user may wish to create co-adds of the spectra observed at each pointing position across all the visits. To co-add these, we can follow the same steps as in [Section 1.4](#coadd1), but set up five different directories for each pointing position. We'll create the directories:\n",
    "| Directory                                                                                                             | Contains Datasets |\n",
    "|--------------------------------------------------------------------------------------------------------------------|----------------|\n",
    "| `newcoadddata_p50`                                                                                    | `oen702010_x1d.fits`, `oen703010_x1d.fits`, `oen704010_x1d.fits`, `oen705010_x1d.fits`            |\n",
    "| `newcoadddata_p25`                                                                             | `oen702040_x1d.fits`, `oen703040_x1d.fits`, `oen704040_x1d.fits`, `oen705040_x1d.fits`            |\n",
    "| `newcoadddata_p0.0`                                | `oen702030_x1d.fits`, `oen703030_x1d.fits`, `oen704030_x1d.fits`, `oen705030_x1d.fits`            |\n",
    "| `newcoadddata_p-25`                                                                            | `oen702050_x1d.fits`, `oen703050_x1d.fits`, `oen704050_x1d.fits`, `oen705050_x1d.fits`            |\n",
    "| `newcoadddata_p-50`                                                                            | `oen702020_x1d.fits`, `oen703020_x1d.fits`, `oen704020_x1d.fits`, `oen705020_x1d.fits`            |\n",
    "\n",
    "\n",
    "Note that we'll still need to exclude the Visit 01 data that was rejected from the MAST query, as that step is bypassed when we run `coadd` from a local folder. Like in the first example, we add the flag `-k` to the call to turn off the `POSTARG` filtering. This time, we'll also add `-t -99999` to set the flux checking threshold. Setting this to a very large negative number will essentially override the flux filtering that `coadd` performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all files in the directory\n",
    "allfiles = glob.glob(os.path.join(datadir_ex2, '*.fits'))\n",
    "\n",
    "# Sort through all the files based on POSTARG value\n",
    "postvals = ['-0.05', '-0.025', '0.0', '0.025', '0.05']\n",
    "for val in postvals:\n",
    "    # Make a list of files all with the same POSTARG values\n",
    "    postarglist = []\n",
    "    for myfile in allfiles:\n",
    "        postarg = fits.getval(myfile, 'POSTARG1')\n",
    "        visitid = fits.getval(myfile, 'OBSET_ID')\n",
    "        if (str(postarg) == val) and (visitid != '01'):\n",
    "            postarglist.append(myfile)\n",
    "\n",
    "    # Make new directories for each list\n",
    "    finaldatadir = os.path.join(datadir_ex2, f'newcoadddata_p{val}')\n",
    "    os.makedirs(finaldatadir, exist_ok=True)\n",
    "\n",
    "    # Copy this list into a new directory to coadd from\n",
    "    for file in postarglist:\n",
    "        print(f'Copying {file} to {finaldatadir}')\n",
    "        shutil.copy(file, finaldatadir)\n",
    "\n",
    "    # Create output directories for the new coadds\n",
    "    productdir = os.path.join(datadir_ex2, f'newcoadddata_p{val}', 'products')\n",
    "    os.makedirs(productdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `coadd` for all the new directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!swrapper -i ./16655/newcoadddata_p-0.05 -o ./16655/newcoadddata_p-0.05/products -k -t -999\n",
    "!swrapper -i ./16655/newcoadddata_p-0.025 -o ./16655/newcoadddata_p-0.025/products -k -t -999\n",
    "!swrapper -i ./16655/newcoadddata_p0.0 -o ./16655/newcoadddata_p0.0/products -k -t -999\n",
    "!swrapper -i ./16655/newcoadddata_p0.025 -o ./16655/newcoadddata_p0.025/products -k -t -999\n",
    "!swrapper -i ./16655/newcoadddata_p0.05 -o ./16655/newcoadddata_p0.05/products -k -t -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that data products have been made for all the datasets, we can plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the older plot\n",
    "plt.close()\n",
    "plt.figure(4)\n",
    "\n",
    "# Set sample factor to 2, which is size of STIS resolution element\n",
    "samplefactor = 2\n",
    "\n",
    "# Plot the coadds\n",
    "for val in postvals:\n",
    "    coadd_filename = 'hst_16655_stis_hd39801_e230m_oen7_cspec.fits'\n",
    "    coaddfile = f\"{datadir_ex2}/newcoadddata_p{val}/products/{coadd_filename}\"\n",
    "    label = f\"{str(float(val) * 10)} mas\"\n",
    "\n",
    "    coadddata = fits.getdata(coaddfile)\n",
    "    wavelength = coadddata['wavelength'][0]\n",
    "    flux = coadddata['flux'][0]\n",
    "\n",
    "    plt.plot(downsample_1d(wavelength, samplefactor),\n",
    "             downsample_1d(flux, samplefactor),\n",
    "             label=label,\n",
    "             alpha=0.7)\n",
    "\n",
    "    targ = fits.getval(coaddfile, 'TARGNAME', ext=0)\n",
    "    pid = fits.getval(coaddfile, 'PROPOSID', ext=0)\n",
    "    ins = fits.getval(coaddfile, 'INSTRUME', ext=0)\n",
    "\n",
    "    plt.title(f'{targ} - {ins} - PID {pid}')\n",
    "    plt.xlabel(r'Wavelength [$\\AA$]')\n",
    "    plt.ylabel(r'Flux [$erg\\ s^{-1}\\ cm^{-2}\\ \\AA^{-1}$]')\n",
    "    plt.legend()\n",
    "\n",
    "# Show the plot below\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot the SNR\n",
    "# Clear the older plot\n",
    "plt.close()\n",
    "plt.figure(5)\n",
    "\n",
    "# Plot the coadds\n",
    "for val in postvals:\n",
    "    coadd_filename = 'hst_16655_stis_hd39801_e230m_oen7_cspec.fits'\n",
    "    coaddfile = f\"{datadir_ex2}/newcoadddata_p{val}/products/{coadd_filename}\"\n",
    "    label = f\"{str(float(val) * 10)} mas\"\n",
    "\n",
    "    coadddata = fits.getdata(coaddfile)\n",
    "    wavelength = coadddata['wavelength'][0]\n",
    "    snr = coadddata['snr'][0]\n",
    "    plt.plot(downsample_1d(wavelength, samplefactor),\n",
    "             downsample_1d(snr, samplefactor),\n",
    "             alpha=0.7,\n",
    "             label=label)\n",
    "\n",
    "    targ = fits.getval(coaddfile, 'TARGNAME', ext=0)\n",
    "    pid = fits.getval(coaddfile, 'PROPOSID', ext=0)\n",
    "    ins = fits.getval(coaddfile, 'INSTRUME', ext=0)\n",
    "\n",
    "    plt.title(f'Betelgeuse - {ins} - PID {pid}')\n",
    "    plt.ylim(0, 300)\n",
    "    plt.xlabel(r'Wavelength [$\\AA$]')\n",
    "    plt.ylabel(r'Signal-to-Noise')\n",
    "    plt.legend()\n",
    "\n",
    "# Show the plot below\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these plots that the flux and SNR decrease the further away from the center of the star you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = stisflux></a>\n",
    "# Example 3: A STIS dataset with flux rejection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = data3></a>\n",
    "### 3.1 Obtaining Data Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next example, we will at Program ID 16196, a COS and STIS program that observed [MRK-817](https://simbad.cds.unistra.fr/simbad/sim-basic?Ident=mrk+817&submit=SIMBAD+search).\n",
    "\n",
    "We will again use `astroquery` to download the dataproducts for this program. We will create a folder called `./16655` for the 1D extracted COS and STIS spectra (`x1ds` and `sx1s`) for all visits in this program, as well as the `x1dsums` for the COS data. We will also create a subfolder called `products`, which will store the downloaded coadded data. The log file for this program is named `HASP_16196.out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories for our data and coadded products\n",
    "datadir_ex3 = Path(\"./16196/\")\n",
    "productsdir_ex3 = Path(\"./16196/products/\")\n",
    "\n",
    "datadir_ex3.mkdir(exist_ok=True)\n",
    "productsdir_ex3.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying and downloading calibrated products\n",
    "query_ex3 = Observations.query_criteria(\n",
    "    proposal_id=16196, \n",
    "    provenance_name=[\"CALSTIS\", \"CALCOS\"],\n",
    "    target_name=\"MRK-817\"\n",
    ")\n",
    "\n",
    "prodlist_ex3 = Observations.get_product_list(\n",
    "    query_ex3\n",
    ")\n",
    "\n",
    "prodlist_ex3 = Observations.filter_products(\n",
    "    prodlist_ex3,\n",
    "    project=[\"CALSTIS\", \"CALCOS\"],\n",
    "    productSubGroupDescription=[\"X1D\", \"X1DSUM\", \"SX1\"]\n",
    ")\n",
    "\n",
    "# Querying and downloading coadded products\n",
    "query_ex3_coadds = Observations.query_criteria(\n",
    "    proposal_id=16196, \n",
    "    provenance_name=\"HASP\",\n",
    "    target_name=\"MRK-817\"\n",
    ")\n",
    "\n",
    "prodlist_ex3_coadds = Observations.get_product_list(\n",
    "    query_ex3_coadds\n",
    ")\n",
    "\n",
    "prodlist_ex3_coadds = Observations.filter_products(\n",
    "    prodlist_ex3_coadds, \n",
    "    productType=\"SCIENCE\",\n",
    "    productSubGroupDescription=\"CSPEC\"\n",
    ")\n",
    "\n",
    "# Combining the two product lists\n",
    "combined_ex3 = vstack([prodlist_ex3, prodlist_ex3_coadds])\n",
    "\n",
    "# Downloading the products\n",
    "Observations.download_products(\n",
    "    combined_ex3,\n",
    "    download_dir=str(datadir_ex3)\n",
    ")\n",
    "\n",
    "# Organizing the files \n",
    "consolidate_files(datadir_ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use `glob` to make a list of every x1d file available in MAST for this PID\n",
    "# Then print out a table of some table info on the program\n",
    "# Note that this program has both x1d and sx1 extracted data products!\n",
    "allfiles = glob.glob(os.path.join(datadir_ex3, '*_x1d.fits'))\\\n",
    "         + glob.glob(os.path.join(datadir_ex3, '*_sx1.fits'))\n",
    "print('rootname  target  visit  grating')\n",
    "for myfile in sorted(allfiles):\n",
    "    hdr0 = fits.getheader(myfile, ext=0)\n",
    "    print(hdr0['rootname'], hdr0['targname'],\n",
    "          hdr0['obset_id'], hdr0['opt_elem'])\n",
    "print(f'N files from MAST = {len(allfiles)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = logs3></a>\n",
    "### 3.2 Examining Output Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up path to the coadd log file\n",
    "logfile = './logfiles/HASP_16196.out'\n",
    "\n",
    "with open(logfile, 'r') as f:\n",
    "    for line in f:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use find_rejects function from first example to make a list of rejected files\n",
    "listofprerejects, listoffluxrejects = find_rejects(logfile, '16196', allfiles)\n",
    "print('Files removed before co-addition:')\n",
    "[print(f\"{file}\") for file in sorted(listofprerejects)]\n",
    "print(f'Files removed by flux checker: {sorted(listoffluxrejects)}')\n",
    "print(f'Number of files removed before co-addition = {len(listofprerejects)}')\n",
    "print(f'Number of files removed by flux checker = {len(listoffluxrejects)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, many files were removed before the co-add began, and many more were removed by the flux checker! If you're curious, we can also look through the headers for the rejected files too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check out the headers of the files for quality\n",
    "readheaders(listofprerejects, datadir_ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a glance at the output in the cell above, we can see that many files have data quality issues. We can create a co-add with the data quality filtering left on, but the flux checker turned off. These types of data products can be useful in cases where the science doesn't depend on the accuracy of the data's absolute flux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = coadd3></a>\n",
    "### 3.3 Running `coadd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to co-add everything again, just without the flux filtering, we don't need to make a new data sub-directory. We should change the output directory name so that the original co-adds are not overwritten. We run `coadd` similarly to [Section 1.4](#coadd1), but this time with just the `-t -999` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the path to your data directory\n",
    "finaldatadir = os.path.join(datadir_ex3, 'products_nofluxfilter')\n",
    "\n",
    "# Make on output directory\n",
    "os.makedirs(finaldatadir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run `coadd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!swrapper -i ./16196/ -o ./16196/products_nofluxfilter -t -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the number of datasets, this co-add takes much longer to compute. Once it's done, we can plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the older plot\n",
    "plt.close()\n",
    "plt.figure(6)\n",
    "\n",
    "fn = 'hst_16196_cos-stis_mrk-817_g130m-g160m-sg230l-g430l-g750l_lede_cspec.fits'\n",
    "\n",
    "# Plot the old coadd\n",
    "oldcoaddfile = os.path.join(datadir_ex3, 'products', fn)\n",
    "oldcoadddata = fits.getdata(oldcoaddfile)\n",
    "oldwavelength = oldcoadddata['wavelength'][0]\n",
    "oldflux = oldcoadddata['flux'][0]\n",
    "\n",
    "plt.plot(downsample_1d(oldwavelength, samplefactor),\n",
    "         downsample_1d(oldflux, samplefactor),\n",
    "         label='coadd w/ flux filter',\n",
    "         color=\"red\",\n",
    "         alpha=0.7)\n",
    "\n",
    "# Plot the new coadd\n",
    "newcoaddfile = os.path.join(datadir_ex3, 'products_nofluxfilter', fn)\n",
    "newcoadddata = fits.getdata(newcoaddfile)\n",
    "newwavelength = newcoadddata['wavelength'][0]\n",
    "newflux = newcoadddata['flux'][0]\n",
    "\n",
    "plt.plot(downsample_1d(newwavelength, samplefactor),\n",
    "         downsample_1d(newflux, samplefactor),\n",
    "         label='coadd w/o flux filter',\n",
    "         color=\"blue\",\n",
    "         alpha=0.7)\n",
    "\n",
    "targ = fits.getval(coaddfile, 'TARGNAME', ext=0)\n",
    "pid = fits.getval(coaddfile, 'PROPOSID', ext=0)\n",
    "ins = fits.getval(coaddfile, 'INSTRUME', ext=0)\n",
    "\n",
    "plt.title(f'{targ} - {ins} - PID {pid}')\n",
    "plt.xlabel(r'Wavelength [$\\AA$]')\n",
    "plt.ylabel(r'Flux [$erg\\ s^{-1}\\ cm^{-2}\\ \\AA^{-1}$]')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot below\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that the flux is lower in the dataset made without the flux checker on. This means that the previously-rejected datasets were of lower flux. However, zooming in on some of the absorption features, we can see that the wings of some lines are broader in the dataset with no flux rejection, which can be useful in some science cases where these features are faint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy co-adding!\n",
    "\n",
    "### There are more tutorial notebooks for custom co-addition cases in [this repo](https://github.com/spacetelescope/hst_notebooks/tree/main/notebooks/HASP/Setup), check them out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this Notebook\n",
    "**Author:** Elaine Frazer (efrazer@stsci.edu), Sierra Gomez (sigomez@stsci.edu)\n",
    "\n",
    "**Updated on:** 02/26/2024\n",
    "\n",
    "*This tutorial was generated to be in compliance with the [STScI style guides](https://github.com/spacetelescope/style-guides) and would like to cite the [Jupyter guide](https://github.com/spacetelescope/style-guides/blob/master/templates/example_notebook.ipynb) in particular.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "If you use `astropy`, `astroquery`, `numpy`, or `matplotlib`, for published research, please cite the authors. Follow these links for more information about citations:\n",
    "\n",
    "* [Citing `astropy`](https://docs.astropy.org/en/stable/index.html)\n",
    "\n",
    "* [Citing `astroquery`](https://astroquery.readthedocs.io/en/latest/)\n",
    "\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/citing.html)\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/citing-numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
